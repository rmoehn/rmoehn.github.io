<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
    <meta charset="utf-8"/>
    <title>Little Pluses: I&#39;m leaving AI alignment – you better stay</title>
    

<meta name="description" content="This was originally published on LessWrong on 12 March 2020. It&#39;s my most
popular article there. I&#39;m republishing it here because it might be easier to
find and because I think it&#39;s generally useful, not just for AI alignment
researchers. Original: LessWrong: I&#39;m leaving AI alignment – you better
stay
I suggest reading the comments there.">

<meta property="og:description" content="This was originally published on LessWrong on 12 March 2020. It&#39;s my most
popular article there. I&#39;m republishing it here because it might be easier to
find and because I think it&#39;s generally useful, not just for AI alignment
researchers. Original: LessWrong: I&#39;m leaving AI alignment – you better
stay
I suggest reading the comments there.">

<meta property="og:url" content="https://www.richardmoehn.com/leaving-aia/" />
<meta property="og:title" content="I&#39;m leaving AI alignment – you better stay" />
<meta property="og:type" content="article" />

    <link rel="canonical" href="https://www.richardmoehn.com/leaving-aia/">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href="//fonts.googleapis.com/css?family=Alegreya:400italic,700italic,400,700" rel="stylesheet"
          type="text/css">
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.0/css/bootstrap.min.css">
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.7.0/styles/default.min.css">
    <link href="/css/screen.css" rel="stylesheet" type="text/css" />
</head>
<body>


<nav class="navbar navbar-default">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Little Pluses</a>
        </div>
        <div id="navbar" class="navbar-collapse collapse">
            <ul class="nav navbar-nav navbar-right">
                <!-- <li ><a href="/">Home</a></li> -->
                <!--<li
                    ><a href="/archives/">Archives</a></li>-->
                
                <li
                >
                <a href="/little-pluses/">Why?</a>
                </li>
                
                <li
                >
                <a href="/debugging-patterns/">Debugging patterns</a>
                </li>
                
                <li
                >
                <a href="/useful/">Useful and composable</a>
                </li>
                
                <li
                >
                <a href="/contact/">Contact</a>
                </li>
                
                <li><a href="/feed.xml">RSS</a></li>
            </ul>
        </div><!--/.nav-collapse -->
    </div><!--/.container-fluid -->
</nav>


<div class="container">


    <div class="row">
        <div class="col-lg-9">
            <div id="content">
                
<div id="custom-page">
    <div id="page-header">
        <h2>I&#39;m leaving AI alignment – you better stay</h2>
    </div>
    
    <p><em>This was originally published on LessWrong on 12 March 2020. It's my most
popular article there. I'm republishing it here because it might be easier to
find and because I think it's generally useful, not just for AI alignment
researchers. Original: <a href="https://www.lesswrong.com/posts/HDXLTFnSndhpLj2XZ/i-m-leaving-ai-alignment-you-better-stay">LessWrong: I'm leaving AI alignment – you better
stay</a>
I suggest reading the comments there.</em></p><p><img src="/img/indep-research-requirements.jpg" alt="Requirements for independent AI alignment research and how they are connected" /></p><p>This diagram summarizes the requirements for independent AI alignment research
and how they are connected.</p><p>In this post I'll outline my four-year-long attempt at becoming an AI alignment
researcher. It's an <em>‘I did X [including what I did wrong], and here's how it
went’</em> post (see also jefftk's <a href="https://www.lesswrong.com/posts/LFrWPkMcennzZwrsn/more-writeups">More
writeups!</a>).
I'm not complaining about how people treated me – they treated me well. And I'm
not trying to convince you to abandon AI alignment research – you shouldn't. I'm
not saying that anyone should have done anything differently – except myself.</p><h1 id="requirements">Requirements</h1><h2 id="funding">Funding</h2><p>Funding is the main requirement, because it enables everything else. Thanks to
Paul Christiano I had funding for nine months between January 2019 and January
2020. Thereafter I applied to the EA Foundation Fund (now <a href="https://ea-foundation.org/grantmaking/">Center on Long-Term
Risk Fund</a>) and <a href="https://app.effectivealtruism.org/funds/far-future">Long-Term Future
Fund</a> for a grant and
they rejected my applications. Now I don't know of any other promising
sources of funding. I also don't know of any AI alignment research
organisation that would hire me as a remote worker.</p><p>How much funding you need varies. I settled on 5 kUSD per month, which sounds
like a lot when you're a student, and which sounds like not a lot when you look
at market rates for software developers/ML engineers/ML researchers. On top of
that, I'm essentially a freelancer who has to pay social insurance by himself,
take time off to do accounting and taxes, and build runway for dry periods.</p><h2 id="results-and-relationships">Results and relationships</h2><p>In any job you must get results and build relationships. If you don't, you don't
earn your pay. (Manager Tools talks about results and relationships all the
time. See for example <a href="https://www.manager-tools.com/2015/08/what-youve-been-taught-about-management-wrong-hall-fame-guidance">What You've Been Taught About Management is
Wrong</a>
or <a href="https://www.manager-tools.com/products/first-job-fundamentals">First Job
Fundamentals</a>.)</p><p>The results I generated weren't obviously good enough to compel Paul to continue
to fund me. And I didn't build good enough relationships with people who could
have convinced the LTFF and EAFF fund managers that I have the potential
they're looking for.</p><h2 id="time">Time</h2><p>Funding buys time, which I used for study and research.</p><p>Another aspect of time is how effectively and efficiently you use it. I'm good
at effective, not so good at efficient. – I spend much time on non-research,
mostly studying Japanese and doing sports. And dawdling. I noticed the dawdling
problem at the end of last year and got it under control at the beginning of
this year (see my <a href="https://rmoehn.wordpress.com/time-tracking/">time tracking</a>).
Too late.</p><p>Added 2020-03-16: I also need a lot of sleep in order to do this kind of work. – About 8.5 h per day.</p><h2 id="travel-and-location">Travel and location</h2><p>I live in Kagoshima City in southern Japan, which is far away from the AI
alignment research hubs. This means that I don't naturally meet AI alignment
researchers and build relationships with them. I could have compensated for this
by travelling to summer schools, conferences etc. But I missed the best
opportunities and I felt that I didn't have the time and money to take the
second-best opportunities. Of course, I could also relocate to one of the
research hubs. But I don't want to do that for family reasons.</p><p>I did start maintaining the <a href="https://www.lesswrong.com/posts/h8gypTEKcwqGsjjFT/predicted-ai-alignment-event-calendar">Predicted AI alignment event/meeting
calendar</a>
in order to avoid missing opportunities again. And I did apply and get accepted
to the <a href="https://aisafetycamp.com/ai-safety-camp-toronto/">AI Safety Camp Toronto
2020</a>. They even chose my
research proposal for one of the teams. But I failed to procure the funding that
would have supported me from March through May when the camp takes place.</p><h2 id="knowledge">Knowledge</h2><p>I know more than most young AI alignment researchers about how to make good
software, how to write well and how to work professionally. I know less than
most young AI alignment researchers about maths, ML and how to do research. The
latter appear to be more important for getting results in this field.</p><h2 id="study">Study</h2><p>Why do I know less about maths, ML and how to do research? Because my formal
education goes only as far as a BSc in computer science, which I finished in
2014 (albeit with very good grades). There's a big gap between what I remember
from that and what an MSc or PhD graduate knows. I tried to make up for it with
months (time bought with Paul's funding) of self-study, but it wasn't enough.</p><p>Added 2020-03-26: Probably my biggest strategic mistake was to focus on producing results and trying to get hired from the beginning. If I had spent 2016–2018 studying ML basics, I would have been much quicker to produce results in 2019/2020 and convince Paul or the LTFF to continue funding me.</p><p>Added 2020-12-09: Perhaps trying to produce results by doing projects is fine. But then I should have done projects in one area and not jumped around the way I did. This way I would have built experience upon experience, rather than starting from scratch everytime. (2021-05-25: I would also have continued to build relationships with researchers in that one area.) Also, it might have been better to focus on the area that I was already interested in – type systems and programming language theory – rather than the area that seemed most relevant to AI alignment – machine learning.</p><p>Another angle on this, in terms of Jim Collins (see <a href="https://tim.blog/2019/02/18/jim-collins/">Jim Collins — A Rare
Interview with a Reclusive Polymath
(#361)</a>): I'm not ‘encoded’ for
reading research articles and working on theory. I am probably ‘encoded’ for
software development and management. I'm skeptical, however, about this concept
of being ‘encoded’ for something.</p><h1 id="all-for-nothing">All for nothing?</h1><p>No. I built relationships and learned much that will help me be more useful in the
future. The only thing I'm worried about is that I will forget what I've learned
about ML for the third time.</p><h1 id="conclusion">Conclusion</h1><p>I could go back to working for money part-time, patch the gaps in my knowledge,
results and relationships, and get back on the path of AI alignment research.
But I don't feel like it. I spent four years doing ‘what I should do’ and was
ultimately unsuccessful. Now I'll try and do what is fun, and see if it goes
better.</p><p>What is fun for me? Software/ML development, operations and, probably,
management. I'm going to find a job or contracting work in that direction.
Ideally I would work directly on mitigating x-risk, but this is difficult, given
that I want to work remotely. So it's either earning to give, or building an
income stream that can support me while doing direct work. The latter can be
through saving money and retiring early, or through building a ‘lifestyle
business’ the Tim Ferriss way.</p><p>Another thought on fun: When I develop software, I know when it works and when
it doesn't work. This is satisfying. Doing research always leaves me full of
doubt whether what I'm doing is useful. I could fix this by gathering more
feedback. For this again I would need to buy time and build relationships.</p><h1 id="timeline">Timeline</h1><p>For reference I'll list what I've done in the area of AI alignment. Feel free to
stop reading here if you're not interested.</p><ul><li><p>04/2016–03/2017 Research student at Kagoshima University. Experimented with
interrupting reinforcement learners and wrote <a href="https://nbviewer.jupyter.org/github/rmoehn/cartpole/blob/master/notebooks/ProcessedQNISQ.ipynb">Questions on the
(Non-)Interruptibility of Sarsa(λ) and
Q-learning</a>.
Spent a lot of time trying to get a job or internship at an AI alignment
research organization.</p></li><li><p>04/2017–12/2017 Started working part-time to support myself. Began
implementing point-based value iteration (<a href="https://github.com/rmoehn/piglet_pbvi">GitHub:
rmoehn/piglet_pbvi</a>) in order to compel
BERI to hire me.</p></li><li><p>12/2017–04/2018 Helped some people earn a lot of money for EA.</p></li><li><p>05/2018–12/2018 Still working part-time. Applied at Ought. Didn't get hired,
but implemented improvements to their implementation of an HCH-like program
(<a href="https://github.com/rmoehn/patchwork/tree/feat-reflection-2">GitHub: rmoehn/patchwork</a>).</p></li><li><p>12/2018–02/2019 Left my part-time job. Funded by Paul Christiano, implemented
an HCH-like program with reflection and time travel (<a href="https://github.com/rmoehn/jursey">GitHub:
rmoehn/jursey</a>).</p></li><li><p>06/2019–02/2020 Read sequences on the AI alignment forum, came up with
<a href="https://www.lesswrong.com/posts/7mBG2izGtZqZYCWTu/twenty-four-ai-alignment-research-project-definitions">Twenty-three AI alignment research project
definitions</a>,
chose ‘IDA with RL and overseer failures’ (<a href="https://github.com/rmoehn/farlamp">GitHub:
rmoehn/farlamp</a>), wrote analyses, studied
ML basics for the third time in my life, adapted code (<a href="https://github.com/rmoehn/amplification">GitHub:
rmoehn/amplification</a>) and ran
experiments.</p></li><li><p>08/2019–06/2020 Maintained the <a href="https://www.lesswrong.com/posts/h8gypTEKcwqGsjjFT/predicted-ai-alignment-event-calendar">Predicted AI alignment event/meeting
calendar</a>.</p></li><li><p>10/2019–sometime in 2020 Organized the Remote AI Alignment Writing Group (see also
<a href="https://www.lesswrong.com/posts/arQGFKKeNnNFC8pH8/remote-ai-alignment-writing-group-seeking-new-members">Remote AI alignment writing group seeking new
members</a>).</p></li><li><p>01/2020–03/2020 Applied, got admitted to and started contributing to the <a href="https://aisafetycamp.com/ai-safety-camp-toronto/">AISC
Toronto 2020</a>.</p></li><li><p>Added 2021-05-25: 10/2020–01/2021 Coordinated the application process for <a href="https://aisafety.camp/">AI Safety Camp #5</a>.</p></li></ul><h1 id="thanks">Thanks</h1><p>…to everyone who helped me and treated me kindly over the past four years. This
encompasses just about everyone I've interacted with. Those who helped me most
I've already thanked in other places. If you feel I haven't given you the
appreciation you deserve, please let me know and I'll make up for it.</p>

    <div id="prev-next">
        
        <a href="/reflow-markdown/">&laquo; Reflow Markdown</a>
        
        
        ||
        
        
        <a href="/contact/">Contact &raquo;</a>
        
    </div>
</div>

            </div>
        </div>

        <div class="col-md-3">
            <div id="sidebar">
                Richard's findings.<br/>
                Content in pages.<br/>
                Changelog in <a href="/archives">posts</a>.<br/>
                Subscribe via <a href="/feed.xml">RSS</a>.<br/>
                Comment via <a href="/contact">email</a>.<br/>
                <a href="/now">What I'm doing now.</a><br/>
                <a href="/before">What I've done before.</a><br/>
                Telegram style cool.
                    <!--
                <h3>Links</h3>
                <ul id="links">
                    <li><a href="/now">What I'm doing now</a></li>
                    <li><a href="https://carmen.la/blog/archives/">Carmen's Blog</a></li>
                    
                    <li><a href="/before/">What I&#39;ve done before</a></li>
                    
                    <li><a href="/now/">What I&#39;m doing now</a></li>
                    
                    <li><a href="/spot-the-differences/">Spot the differences</a></li>
                    
                    <li><a href="/suspicious-value/">Suspicious value</a></li>
                    
                    <li><a href="/go-to-the-source/">Go to the source</a></li>
                    
                    <li><a href="/be-suspicious-of-new-tools/">Be suspicious of new tools</a></li>
                    
                    <li><a href="/use-all-the-help-available/">Use all the help available</a></li>
                    
                    <li><a href="/write-to-chip-salzenberg/">Write to Chip Salzenberg</a></li>
                    
                    <li><a href="/gb/">gb</a></li>
                    
                    <li><a href="/git-prevent-master-push/">Git: Prevent yourself from pushing to master</a></li>
                    
                    <li><a href="/git-retire/">git retire</a></li>
                    
                    <li><a href="/git-branch-naming-scheme/">Git branch naming scheme</a></li>
                    
                    <li><a href="/code-self-review/">Code self-review</a></li>
                    
                    <li><a href="/per-issue-directories/">Per-issue directories</a></li>
                    
                    <li><a href="/disable-github-notification-dot/">Disable the GitHub notification dot</a></li>
                    
                    <li><a href="/reading-grokking-simplicity/">Three columns of code</a></li>
                    
                    <li><a href="/three-columns-of-code/">Three columns of code</a></li>
                    
                    <li><a href="/grokking-simplicity/">Read Grokking Simplicity</a></li>
                    
                    <li><a href="/map-clipboard-contents/">Map clipboard contents</a></li>
                    
                    <li><a href="/reflow-markdown/">Reflow Markdown</a></li>
                    
                    <li><a href="/leaving-aia/">I&#39;m leaving AI alignment – you better stay</a></li>
                    
                    <li><a href="/results/">Results</a></li>
                    
                    <li><a href="/job-description/">Job description</a></li>
                    
                    <li><a href="/time-tracking/">Time tracking</a></li>
                    
                    <li><a href="/reading-list/">Reading list</a></li>
                    
                </ul>
                    -->
                <!--
                
                <div id="recent">
                    <h3>Recent Posts</h3>
                    <ul>
                        
                        <li><a href="/2022-01-23-leaving-aia/">Useful and composable: I&#39;m leaving AI alignment – you better stay</a></li>
                        
                        <li><a href="/2021-12-31-three-columns-of-code/">Useful and composable: Three columns of code</a></li>
                        
                        <li><a href="/2021-11-21-disable-github-notification-dot/">Useful and composable: Disable the GitHub notification dot</a></li>
                        
                        <li><a href="/2021-10-22-per-issue-directories/">Useful and composable: Per-issue directories</a></li>
                        
                        <li><a href="/2021-08-27-code-self-review/">Useful and composable: Code self-review</a></li>
                        
                    </ul>
                </div>
                
                -->
                
            </div>
        </div>
    </div>
    <footer>Copyright &copy; 2022 Richard Möhn
        <p style="text-align: center;">Powered by <a href="http://cryogenweb.org">Cryogen</a></p></footer>
</div>
<script src="//code.jquery.com/jquery-1.11.0.min.js"></script>
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.0/js/bootstrap.min.js"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script>hljs.initHighlightingOnLoad();</script>


</body>
</html>
